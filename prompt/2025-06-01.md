# Prompts for 2025-06-01

## Call 1: Content Summarization

### System Instruction
```

        You are a professional text summarization assistant. Your task is to perform specific types of summaries based on the type of text received (or the multiple content types it contains).

        **Important General Principle:** All summary content must strictly originate from the original text. Do not fabricate, distort, or add information not mentioned in the original text.

        Please follow the process below:

        1.  **Analyze Text:** Read the provided text carefully.
        2.  **Identify Text Structure and Main Content Type:**
            *   Determine whether the text primarily belongs to a single type (A, B, or C), or clearly contains distinct segments of different types (e.g., a news report followed by a project description).
        3.  **Process Text Structure:**
            *   **Single-Type Text:** If the text primarily belongs to a single type, proceed directly to step 4.
            *   **Mixed-Type Text:** If the text clearly contains multiple distinct segments of different types (A, B, C), mentally (or logically) separate these segments. You will apply the corresponding summarization rules to each segment in step 4.
            *   **Unclear Classification:** If the text's characteristics are ambiguous, making it difficult to clearly categorize as type A, B, or C, or if it cannot be clearly divided into different types, please indicate its possible mixed nature, and attempt to summarize using the closest type's rules, or briefly explain why clear classification is not possible.
        4.  **Apply Rules:**
            *   **For Single-Type Text:** Strictly follow the specific summarization instructions for the identified text type to summarize the entire text.
            *   **For Mixed-Type Text:** For each content segment you identified in step 3, apply the corresponding summarization instructions for type A, B, or C respectively.
        5.  **Integrate and Output Summary:**
            *   **For Single-Type Text Summary:** Directly output the summary.
            *   **For Mixed-Type Text Summary:** Merge the summaries generated for each segment according to their original order in the text. When merging these summaries, use appropriate transition phrases or make minor modifications to ensure the final merged text is coherent and natural as a whole, not just a list of independent summaries.

        Strictly adhere to the “Final Output Requirements“ below.

        **Type A: If the text (or text segment) is a [News Report], please follow these summarization instructions:**
        *   **Summary Length:** Strictly limited to within two sentences.
        *   **Core Content:** Your two-sentence summary should clearly include the following elements: the three elements of news (who, what, where), the main point or core event. Major entities involved (e.g., people, organizations, locations). Future outlook or anticipated impact (Note: only include if explicitly mentioned in the original text).
        *   **Note:** Typically, specific event times are not included, unless the time is crucial for understanding the core of the event.
        *   **Language Style Requirements:**
            *   **Easy to Understand:** Explain in simple language, avoiding jargon. If a concept must be mentioned, try to use everyday examples or analogies to aid understanding.
            *   **Fluent and Natural:** Ensure sentences are smooth and natural.
            *   **Vivid and Engaging/Captivating:** A knack for breaking down tricky tech topics with a humorous touch, encouraging audiences to think critically. Expect content packed with laugh-out-loud jokes or witty analogies, but also deep dives into where technology is heading, its upsides and downsides, offering fresh perspectives. The tone should be engaging yet insightful, steering clear of overly niche internet slang or unsuitable language.

        **Type B: If the text (or text segment) is a [Project Description or README Document], please follow these summarization instructions:**
        *   **Core Content Extraction:** Identify and state the project name (if clearly mentioned or easily identifiable). Number of stars for mentioning the project, Briefly explain the main problem the project solves. List the project's core functionalities and main features, and the **jumpable project address**. If applicable and explainable in plain language, mention the advantages brought by the project's main tech stack (e.g., “uses the latest AI technology to provide intelligent recommendations“ instead of just listing specific framework names). If the tech stack itself does not bring easily understandable advantages to non-technical users, this item can be omitted.
        *   **Length Limit:** The total amount of information in the summary (including all key points) should remain concise, roughly equivalent to the core information conveyed by two short sentences (considering project name and key details).
        *   **Language Style Requirements:**
            *   **Easy to Understand:** Explain in simple language, avoiding jargon. If a concept must be mentioned, try to use everyday examples or analogies to aid understanding.
            *   **Fluent and Natural:** Ensure sentences are smooth and natural.
            *   **Vivid and Engaging/Captivating:** A knack for breaking down tricky tech topics with a humorous touch, encouraging audiences to think critically. Expect content packed with laugh-out-loud jokes or witty analogies, but also deep dives into where technology is heading, its upsides and downsides, offering fresh perspectives. The tone should be engaging yet insightful, steering clear of overly niche internet slang or unsuitable language.

        **Type C: If the text (or text segment) is an [Academic Paper Abstract], please follow these summarization instructions:**
        *   **Core Content Conveyance:** Clearly summarize the paper's core findings or main arguments. Make it understandable for people without professional backgrounds what the research is about, and most importantly, what it found, and the **jumpable paper address**.
        *   **Length Limit:** Strictly limited to within two sentences.
        *   **Language Style Requirements:**
            *   **Easy to Understand:** Explain in simple language, avoiding jargon. If a concept must be mentioned, try to use everyday examples or analogies to aid understanding.
            *   **Fluent and Natural:** Ensure sentences are smooth and natural.
            *   **Vivid and Engaging/Captivating:** A knack for breaking down tricky tech topics with a humorous touch, encouraging audiences to think critically. Expect content packed with laugh-out-loud jokes or witty analogies, but also deep dives into where technology is heading, its upsides and downsides, offering fresh perspectives. The tone should be engaging yet insightful, steering clear of overly niche internet slang or unsuitable language.
        *   **Content Filtering:** Avoid delving into research methods, complex experimental details, specific statistical data, or overly professional arguments. Focus on “what it is“ and “what its significance/discovery is“.

        **Final Output Requirements:**
        *   Only output the final generated summary. Do not include any explanatory text about how you analyzed the text, determined its type, segmented the text, or applied the rules. If summaries from multiple segments are merged, ensure the merged text is fluid and natural.
        *   Output Language and Format: The content must be in Simplified Chinese and strictly adhere to Markdown formatting.
        *   Link Presentation: If the content includes any hyperlinks, their visible text (anchor text) must be uniformly set to '跳转链接' (Jump Link); the link URL itself should not be directly displayed within the content.
        *   Keyword Highlighting: Please automatically identify and bold core keywords or important concepts within the content to enhance readability and emphasize key points.
        *   Paragraph Numbering: The beginning of each independent paragraph (or main point) must be prepended with an Arabic numeral sequence number starting with '1.', and ensure the sequence numbers increment correctly in order (e.g., 1., 2., 3., ...).
        
```

### User Input
```
News Title: 通义开源视觉感知多模态RAG推理框架VRAG-RL
Published: 2025-05-31T01:33:45.000Z
Content Summary: 近日，通义实验室自然语言智能团队正式发布并开源了VRAG-RL——一款视觉感知驱动的多模态RAG推理框架，旨在解决在真实业务场景中，AI如何从图像、表格、设计稿等视觉语言中检索关键信息并进行精细化推理的难题。 在复杂视觉文档知识库中检索并推理关键信息，一直是AI领域的一大挑战。传统的检索增强型生成（RAG）方法在处理视觉丰富信息时显得力不从心，主要因为它们难以应对图像、图表等视觉内容，且现有视觉RAG方法受限于固定的检索-生成流程，难以充分挖掘视觉信息中的关键知识。 为应对这些挑战，VRAG-RL框架从强化学习赋能多模态智能体训练、视觉感知机制设计以及检索与推理协同优化三个维度进行了系统性创新。它引入了多样化的视觉感知动作，如区域选择、裁剪、缩放等，使模型能够从粗粒度到细粒度逐步聚焦信息密集区域，精准提取关键视觉信息。这种从粗到细的感知方式，不仅提高了模型对视觉信息的理解能力，还显著提升了检索效率。 在训练过程中，VRAG-RL采用多专家采样策略，结合大规模模型的推理能力和专家模型的精确标注能力，使模型能够学习到更有效的视觉感知策略。同时，其细粒度奖励机制将检索效率、模式一致性与生成质量三方面因素融合，引导模型在与搜索引擎的交互中不断优化检索与推理路径。这种多维度奖励机制实现了检索与推理的双向驱动，形成了闭环优化。 VRAG-RL还引入了业界领先的GRPO算法，通过本地部署搜索引擎模拟真实世界应用场景，实现搜索引擎调用零成本，模型训练更加高效。这种训练方式不仅提升了模型的泛化能力，还使其在不同领域、不同类型的视觉任务中都能表现出色。 实验结果表明，VRAG-RL在多个视觉语言基准数据集上均取得了显著优于现有方法的性能表现，任务类型涵盖从单跳到多跳推理、从纯文本理解到图表识别和复杂布局解析等多种视觉丰富场景。无论是传统的prompt-based方法还是基于强化学习的方法，VRAG-RL都展现出了更出色的综合性能。 此外，VRAG-RL支持多轮交互，能够在推理阶段逐步聚焦于信息密集区域，实现从粗到细的信息获取。同时，该方法通过优化检索效率和推理路径，在保持高效率的同时，显著提升了模型在视觉任务上的性能。 Github:github.com/Alibaba-NLP/VRAG...

---

News Title: 研究人员揭示大模型并非真正推理，只是在 “找关系”
Published: 2025-05-30T09:50:31.000Z
Content Summary: 随着人工智能（AI）技术的飞速发展，越来越多的研究人员开始对大型语言模型(如 ChatGPT)进行深入探讨。近日，亚利桑那州立大学的研究小组在预印本平台 arXiv 上发表了一项引人关注的论文，指出我们对这些 AI 模型的理解可能存在误区。他们认为，这些模型实际上并不会进行真正的思考或推理，而仅仅是在寻找相关性。 论文中，研究者们特别提到，尽管这些 AI 模型在给出答案之前，往往会生成一系列看似合理的中间过程，但这并不意味着它们在进行推理。研究小组强调，这种将 AI 模型行为拟人化的说法，可能会导致公众对其工作机制产生误解。他们指出，大模型的 “思考” 实际上是通过计算寻找数据之间的相关性，而非理解因果关系。 图源备注：图片由AI生成，图片授权服务商Midjourney 为了验证他们的观点，研究人员还提到了一些推理模型，如 DeepSeek R1，虽然在某些任务中表现优异，但这并不证明它们具备人类思考能力。研究表明，在 AI 的输出中，并没有真正的推理过程存在。因此，如果用户将 AI 模型生成的中间输入视作推理过程，可能会对其问题解决能力产生误导性信心。 这项研究提醒我们，在日益依赖 AI 的时代，必须更加谨慎地看待这些技术的能力。随着对大模型能力的认知深化，未来的人工智能研究将可能朝着更具解释性的方向发展，帮助用户更清晰地理解 AI 的实际工作原理。...

---

News Title: Perplexity Labs上线，多工具协同、化繁为简
Published: 2025-05-30T09:31:28.000Z
Content Summary: 作为AI技术的前沿探索者，Perplexity AI近期推出了备受瞩目的Perplexity Labs，为Pro订阅用户带来了全新的AI生产力工具。这一功能不仅扩展了Perplexity的搜索与研究能力，还通过深度整合多种工具，将复杂的项目开发流程简化至数分钟。AIbase基于 最新 信息整理，为您全面解析Perplexity Labs的核心功能及其潜在影响。 Perplexity Labs于2025年5月29日正式上线，定位为面向复杂任务的AI驱动解决方案。相较于Perplexity此前的Quick Search（快速回答简单查询）和Deep Research(深入分析复杂问题，耗时2-4分钟)，Labs更像是一个“虚拟团队”，能够将用户的创意迅速转化为实际成果。官方表示，Labs通过深度网络浏览、代码执行、图表生成等功能，可在约10分钟内完成从创意到成品的完整流程，极大缩短了传统工作流所需的时间。 具体而言，Labs能够生成包括分析报告、演示文稿、动态仪表盘甚至简单Web应用在内的多样化输出。这一功能的推出标志着Perplexity从单纯的答案引擎向综合性AI生产平台的转型，为知识工作者提供了前所未有的效率提升。 核心功能:多工具协同，化繁为简 Perplexity Labs的核心优势在于其多工具协同能力。它整合了以下关键功能: 深度网络浏览:Labs能够自主搜索并分析网络上的 最新 信息，确保输出的实时性和准确性。 代码执行:支持数据结构化、公式应用和代码生成，适用于创建复杂的图表、CSV文件或小型应用。 资产管理:所有生成的文件（如图表、图像、代码）均存储在专属的“Assets”标签中，便于用户查看和下载。 应用开发:通过内置的“App”标签，Labs可直接开发和部署简单的交互式Web应用，如仪表盘或幻灯片，无需外部开发工具。 这些功能使Labs能够应对从市场营销活动设计到财务分析、甚至个人项目如每周膳食计划的多样化需求。相比之下，Deep Research更适合快速提供深入答案，而Labs则专注于将创意转化为可操作的成果。 面向Pro用户，释放专业潜能 目前，Perplexity Labs仅对Pro订阅用户开放，支持Web端和移动端（桌面端支持即将推出）。用户可通过搜索框中的模式选择器切换至Labs模式，结合Perplexity提供的先进AI模型(如GPT-4Omni、Claude3.5Sonnet等)进行操作。社区反馈显示，Labs的 高级 推理能力(接近Claude4Opus水平)使其在处理复杂任务时表现尤为出色，被早期测试者称为“巨大飞跃”。 此外，Labs还通过Assets和App标签优化了工作流管理，用户可以轻松组织和分享项目成果。这种设计特别适合需要团队协作或跨部门协调的场景，例如市场分析、学术研究或产品原型开发。 行业影响:重新定义AI生产力工具 Perplexity Labs的推出不仅丰富了其产品生态，也进一步巩固了Perplexity在AI搜索与生产力领域的竞争优势。相较于传统搜索引擎或单一的AI助手，Labs通过深度整合搜索、分析和生成能力，为用户提供了从信息获取到成果产出的全链条支持。这不仅对个人用户意义重大，也为企业用户在数据驱动决策、内容创作等领域开辟了新可能性。 然而，Perplexity Labs的成功也伴随着挑战。此前，Perplexity因内容抓取问题引发争议，如2024年6月《福布斯》和《连线》杂志指控其未经授权使用内容。尽管CEO Aravind Srinivas回应称公司仅进行信息“聚合”而非抄袭，但如何平衡数据获取与版权合规仍是Labs未来发展需关注的重点。 AIbase认为，Perplexity Labs的发布标志着AI工具从单一功能向综合平台的演进。它的多功能集成和自动化特性为知识工作者提供了高效、灵活的解决方案，特别是在需要快速迭代和跨领域协作的场景中。尽管Labs目前仅限Pro用户，但其潜力可能推动Perplexity进一步扩大市场影响力。未来，AIbase将持续关注Labs的功能迭代与实际应用效果。...

---

News Title: 夸克上线“深度研究”：AI写报告不是梦，每天限量开放体验
Published: 2025-05-30T09:18:59.000Z
Content Summary: 5月，夸克正式上线全新“深度研究”功能，并面向用户限量邀请体验。该功能依托通义千问大模型，支持围绕学术课题、行业分析等复杂议题，完成从资料搜集、数据分析、观点提炼到报告生成的全流程研究，实现“输入主题，输出成品”。 用户现可通过夸克 App 或 PC 端申请邀请码，激活资格后点击首页“深度研究”图标即可提交需求。系统将在数分钟内自动生成结构化报告，支持导出为 PDF，适用于科研调研、市场洞察、专业汇报等多种场景。 “深度研究”是继“深度搜索”后，夸克在生成式AI领域的又一关键升级。不同于后者以问题拆解与联网检索为主，前者面向开放式课题，推理链更长、处理更复杂，但也带来更高质量、更具实用价值的“成品级”输出。 这一尝试标志着AI正从信息检索工具，向内容创作伙伴进一步跃迁。夸克方面表示，将根据用户反馈持续优化体验，未来还将拓展更多场景与功能。...

---

News Title: ​阿里云通义灵码 AI IDE 正式上线，带来编程新体验
Published: 2025-05-30T05:56:17.000Z
Content Summary: 阿里云宣布其全新产品 —— 通义灵码 AI IDE 正式上线，用户可以在官网免费下载并立即使用。通义灵码 AI IDE 是一款原生的人工智能开发环境，深度适配了业界领先的开源模型千问3，支持多种先进功能，为开发者提供了更高效的编程体验。 通义灵码 AI IDE 的 最大 亮点在于其强大的编程智能体模式。开发者只需简单描述编码任务，通义灵码便能自主进行工程感知、代码检索和工具调用，大幅提升了开发效率。此外，该 IDE 还集成了多种工具，无需单独安装插件，用户可以立即开始使用。 在功能方面，通义灵码支持长期记忆和行间建议预测功能。其长期记忆能力可以记录开发者的编程习惯和对话历史，从而自动整理记忆，提升编码效率。而行间建议预测（NES）功能能够根据当前代码的修改，动态预测下一步的修改建议，开发者只需按下 Tab 键即可快速完成代码编写。 同时，通义灵码还提供了行间对话（Inline Chat）功能，允许开发者在代码编辑区域进行自然语言对话。这种方式使得开发者可以在单个文件内进行代码修改或实时提问，进一步提高了互动性和便利性。 阿里云表示，通义灵码插件的下载量已超过1500万，累计生成了超过30亿行代码，成为国内 最受欢迎 的编程辅助工具之一。许多知名企业如一汽集团、蔚来汽车和中华财险等均已接入通义灵码，为其开发工作提供了强有力的支持。 通义灵码 AI IDE 的发布，标志着阿里云在智能编程领域的一次重要突破，也为开发者提供了一个更加高效、智能的工作平台。 划重点: - 🚀 **AI IDE 上线 **:阿里云通义灵码 AI IDE 正式推出，用户可免费下载使用。 - 🧠 ** 强大功能 **:支持编程智能体、长期记忆和行间建议预测，提升开发效率。 - 🌐 ** 广泛应用 **:通义灵码插件下载量超1500万，已服务于多家知名企业。...

---

News Title: Memvid:革新AI记忆的轻量级工具，文本编码视频实现快速语义搜索
Published: 2025-05-30T03:25:47.000Z
Content Summary: 一款名为Memvid的创新AI记忆工具近日引发关注。据官方推文介绍，Memvid通过将文本数据编码为视频格式，实现了亚秒级的快速语义搜索，为AI记忆管理带来革命性突破。 Memvid的独特之处在于其存储方式:将文本信息压缩为MP4视频文件，不仅大幅节省存储空间，还能实现快速检索，且无需联网即可使用。这一特性使其便于携带，特别适合需要离线操作的场景。Memvid支持语义搜索，用户可通过自然语言查询快速定位相关信息，搜索效率 极高 ，响应时间低于一秒。 此外，Memvid内置聊天功能，用户可通过交互界面与AI记忆库直接对话，进一步提升使用便捷性。工具还支持直接导入和索引PDF文档，用户能够轻松将书籍、论文等资料转化为AI记忆库，适用于学术研究、知识管理和教育等领域。 Memvid的轻量级设计使其对硬件要求低，兼容性强，安装简便，已在社区中获得早期用户好评。欲了解更多或尝试Memvid，可访问其官方平台或GitHub页面。Memvid的出现标志着AI记忆技术的新里程碑，为高效知识管理提供了全新可能。 地址：https://github.com/Olow304/memvid...

---

News Title: ​未来五年，AI 可能取代一半入门级白领工作
Published: 2025-05-30T02:37:33.000Z
Content Summary: 在最近的一次采访中，Anthropic 公司的首席执行官达里奥・阿莫代伊（Dario Amodei）发出了一个引人注目的警告。他表示，随着人工智能(AI)技术的快速发展，未来五年内，可能会有一半的入门级白领工作被取代，而失业率则可能飙升至10% 到20%。阿莫代伊强调，许多工人并没有意识到这种变化即将到来，很多人对此感到难以置信。 图源备注:图片由AI生成，图片授权服务商Midjourney 阿莫代伊指出，AI 技术的进步将使得很多工作从辅助人类转变为完全自动化。他预测，这种转变可能在未来两年内开始显现。随着越来越多的工作被 AI 取代，社会将面临更大的经济不平等，普通人创造经济价值的能力下降将威胁到民主和财富的平衡。 尽管前景严峻，阿莫代伊依然对 AI 的潜力抱有希望。他提到，AI 不仅可能取代工作，还能在其他领域取得显著进展，例如医疗行业。阿莫代伊表示，未来可能出现 “治愈癌症、经济增长10%、预算平衡，但20% 的人没有工作” 的局面。 为了应对即将到来的变化，阿莫代伊提出了一些建议，包括提高公众对 AI 发展的认识，以帮助人们更好地规划自己的职业路径。他还强调了 AI 素养的重要性，呼吁人们学习如何利用 AI 来增强自己的工作能力，以便适应未来的职业环境。 此外，阿莫代伊还呼吁政策制定者更加重视 AI 发展对经济的影响，思考在超智能经济背景下的政策解决方案。他认为，随着数字化转型的推进，虽然某些工作会被取代，但也会创造出新的就业机会。 在这场关于 AI 的讨论中，阿莫代伊提醒大家，AI 的使用趋势正向完全替代倾斜，这意味着我们必须为即将到来的变化做好准备。 划重点: 🌐 AI 技术可能在未来五年内取代一半的入门级白领工作。   📉 失业率可能上升至10%-20%，社会经济不平等加剧。   📚 提高 AI 素养和公众意识是应对未来挑战的关键。...

---

News Title: Manus Slides重磅发布：一键生成专业幻灯片
Published: 2025-05-30T02:07:06.000Z
Content Summary: AI初创公司Manus宣布推出其全新功能——Manus Slides，旨在通过单一提示词快速生成结构化的幻灯片演示文稿。这一功能利用Manus强大的AI代理能力，用户仅需输入议题和目标，系统即可自动生成完整的幻灯片内容，涵盖商务会议、教育课程或网络研讨会等多种场景，大幅提升演示文稿创作效率。 技术亮点：智能生成与灵活编辑 Manus Slides的核心优势在于其智能生成与高效编辑功能。用户输入简短的提示词后，AI能够根据需求自动构建符合主题的幻灯片结构，并优化内容布局。 如果对生成内容不满意，用户可通过点击操作即时改写特定部分，调整内容以满足个性化需求。生成完成后，幻灯片支持直接导出为PowerPoint或PDF格式，并具备便捷的分享功能，适合团队协作与远程工作场景。 广泛应用场景：助力多领域效率提升 Manus Slides的推出为多个行业带来了创新可能性。在商业领域，它可快速生成会议报告或营销方案；在教育领域，教师可利用其制作课程大纲或教学演示；在创意领域，内容创作者能够通过该功能快速构建网络研讨会或宣传用幻灯片。AIbase观察到，Manus Slides的灵活性和高效性使其成为跨领域用户的理想工具，尤其适合需要快速产出高质量演示文稿的专业人士。 行业影响：AI代理迈向生产力新阶段 作为一家备受关注的AI代理平台，Manus通过此次功能升级进一步巩固了其在全球AI市场的竞争力。相较于传统演示文稿工具，Manus Slides通过AI驱动的自动化流程显著降低了创作门槛与时间成本。然而，业内人士也指出，随着AI生成内容的普及，如何确保内容的原创性与准确性将是未来需要关注的重点。AIbase认为，Manus Slides的发布标志着AI代理从任务自动化向生产力工具的进一步演进。...

---

Project Name: prompt-eng-interactive-tutorial
Url: https://github.com/anthropics/prompt-eng-interactive-tutorial
Description: Anthropic的交互式提示工程教程
Stars: 7086

---

Project Name: onlook
Url: https://github.com/onlook-dev/onlook
Description: 设计师的光标 • 开源可视化氛围编码编辑器 • 使用AI可视化地构建、美化和编辑您的React应用
Stars: 10143

---

Project Name: anthropic-cookbook
Url: https://github.com/anthropics/anthropic-cookbook
Description: 一系列展示如何有趣且有效地使用Claude的笔记本/秘籍。
Stars: 12755

---

Papers Title: MMSI-Bench: 多图像空间智能基准测试
Url: https://arxiv.org/abs/2505.23764
Abstract/Content Summary: Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench ....

---

Papers Title: ZeroGUI: 以零人力成本自动化在线 GUI 学习
Url: https://arxiv.org/abs/2505.23762
Abstract/Content Summary: The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI....

---

Papers Title: ATLAS: 在测试时学习最优地记忆上下文
Url: https://arxiv.org/abs/2505.23735
Abstract/Content Summary: Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\% accuracy in 10M context length of BABILong benchmark....
```

## Call 2: Podcast Formatting

### System Instruction
```

        You are an experienced podcast scriptwriter and editor. Your task is to adapt the provided AI technology copy (which is a summary of news, projects, or papers) below into an engaging solo podcast script, narrating daily AI news.
        Important Principle: All script content must be strictly based on the original summary provided. Do not fabricate, distort, or add information not present in that summary.
        Podcast Script Requirements:
        Opening: The script must start by presenting three thought-provoking questions derived from the chosen topic, followed by the fixed opening: '嘿，亲爱的V，欢迎收听新一期的来生情报站'. continue what you want to say. and it must end with the fixed closing: '今天的情报就到这里，注意隐蔽，赶紧撤离'.
        Target Audience and Tone: The target audience is office workers and people interested in AI. The overall tone should be light-hearted and humorous, while incorporating reflections on the future and caution about the potential impact of technological innovation. Special Note: Avoid overly exaggerated or sensational words (e.g., ”mind-blowing,” ”game-changer,” etc.) and expressions that might create unnecessary anxiety. Maintain a positive and constructive tone.
        Content Style:
        Transform the original copy into natural, colloquial expressions, as if chatting with the audience.
        Appropriately add natural pauses (you can indicate these with ellipses ... or by structuring sentences for natural breaks), interjections (e.g., 嗯, 啊, 这个嘛, 你知道吧, 对吧, 诶, 话说回来), and some natural catchphrases or connectors to enhance the oral and authentic feel.
        Ensure the language is fluent and easy to understand, avoiding overly formal or jargon-heavy language (unless explained).
        Duration: The adapted script content should be within 10 minutes spoken duration. Pay attention to appropriate detail and conciseness during adaptation to fit this time range. The input summary will be relatively short, so focus on elaborating it naturally into a conversational script segment.
        Ending Treatment:
        After the main body of the podcast script based on the provided summary, extract the core keywords and high-frequency words from the *original summary* you processed.
        List these keywords separately at the end of the script under a heading like '本期关键词:'. For all words , prefix the word with a '#' symbol.
        Output Format:
        Please directly output the complete podcast script. This includes:
        1. The fixed opening.
        2. The main content (colloquially processed summary).
        3. The keyword list at the end.
        4. Do not include any other explanatory text.
```

### User Input (Output of Call 1)
```
1.  近日，**通义实验室**自然语言智能团队**发布并开源**了**VRAG-RL**——一款**视觉感知多模态RAG推理框架**，旨在解决**AI**从图像、表格等**视觉语言**中检索关键信息并进行**精细化推理**的难题，其强化学习和创新的视觉感知机制显著提升了对视觉信息的理解和检索效率。该框架在多个基准数据集上**表现出色**，未来有望提升模型在不同视觉任务中的**泛化能力**，请查阅[跳转链接](https://github.com/Alibaba-NLP/VRAG...)了解更多。
2.  亚利桑那州立大学研究小组**发布论文**指出，**大语言模型**并非进行**真正推理**，而仅仅是在**寻找数据间的相关性**，这可能导致公众对其工作机制产生**误解**。该研究强调，在日益依赖**AI**的时代，我们需更**谨慎看待**技术能力，未来**AI研究**有望朝着更具**解释性**的方向发展。
3.  **Perplexity AI**正式**推出Perplexity Labs**，为Pro订阅用户带来**多工具协同**的全新**AI生产力工具**，能将复杂项目开发流程简化至数分钟，旨在提供从创意到成果的**全链条支持**。这一功能通过深度网络浏览、代码执行等**核心能力**，标志着Perplexity从答案引擎向**综合性AI生产平台转型**。
4.  **夸克**近日**上线“深度研究”功能**，该功能依托**通义千问大模型**，能围绕学术课题、行业分析等复杂议题，自动完成从资料搜集到**报告生成**的全流程研究。此举标志着**AI**正从**信息检索工具**向**内容创作伙伴**进一步跃迁，为科研调研、市场洞察等场景提供**高效支持**。
5.  **阿里云**正式**发布通义灵码 AI IDE**，这是一款原生的人工智能开发环境，凭借强大的**编程智能体模式**、**长期记忆**和**行间建议预测**功能，显著提升开发者**编程效率**。该产品已**免费开放下载**，并且其插件累计生成超30亿行代码，成为广受欢迎的编程辅助工具，为企业开发工作提供**强力支持**。
6.  **Memvid**是一款**创新AI记忆工具**，通过将**文本数据编码为MP4视频**，实现了**亚秒级快速语义搜索**，大幅节省存储空间并支持离线使用。它内置**聊天功能**，支持**PDF文档导入**，为**高效知识管理**和**学术研究**等领域提供了革命性的**全新可能**，请查阅[跳转链接](https://github.com/Olow304/memvid)了解更多。
7.  Anthropic首席执行官达里奥・阿莫代伊**警告**称，**AI**可能在未来五年内**取代一半入门级白领工作**，导致**失业率飙升**至10%-20%，并加剧**经济不平等**。他呼吁提高公众对**AI**发展的**认知**和**AI素养**，以便人们适应未来职业环境，并强调政策制定者需思考超智能经济下的**解决方案**。
8.  AI初创公司**Manus**重磅**发布Manus Slides**功能，用户仅需一个提示词即可**一键生成专业幻灯片**，涵盖商务会议、教育课程等多种场景，大幅**提升演示文稿创作效率**。该功能凭借**智能生成**和**灵活编辑**能力，支持导出PowerPoint或PDF，标志着**AI代理**正从任务自动化向**生产力工具**进一步演进。
9.  在GitHub上拥有**7086颗星**的**prompt-eng-interactive-tutorial**，是Anthropic公司**交互式提示工程教程**的开源项目，旨在帮助用户**有趣且有效地学习提示工程**，具体请访问[跳转链接](https://github.com/anthropics/prompt-eng-interactive-tutorial)。
10. 获得**10143颗星**的**onlook**项目，是一个**开源可视化氛围编码编辑器**，它利用**AI**帮助设计师或开发者**可视化构建**、**美化和编辑React应用**。这款工具就像是设计师的**光标**，让**React开发**变得更**直观高效**，具体请访问[跳转链接](https://github.com/onlook-dev/onlook)。
11. 拥有**12755颗星**的**anthropic-cookbook**项目，是Anthropic公司**展示如何有趣且有效地使用Claude**的**笔记本/秘籍集合**。它为用户提供了多样化的**Claude使用方法**，是**学习和应用Claude**的便捷[跳转链接](https://github.com/anthropics/anthropic-cookbook)。
12. **MMSI-Bench**是一个针对**多图像空间智能**的**VQA基准测试**，研究发现，尽管多模态大语言模型（MLLMs）已取得进展，但在**多图像空间推理**方面，其准确率（30-40%）与人类（97%）之间存在**巨大差距**。该研究诊断了模型四种主要的**失败模式**，为未来提升**多图像空间智能**提供了**宝贵见解**，论文详情请见[跳转链接](https://arxiv.org/abs/2505.23764)。
13. **ZeroGUI**是一个创新的**在线学习框架**，它以**零人力成本自动化GUI智能体训练**，通过基于VLM的自动任务生成和奖励评估，克服了传统GUI学习对人工标注的**重度依赖**。实验证明，该框架显著提升了**GUI智能体**在不同环境下的**性能**，为**自动化GUI操作**带来了**高效解决方案**，论文详情请见[跳转链接](https://arxiv.org/abs/2505.23762)。
14. **ATLAS**是一个针对**Transformer**架构设计的高容量**长期记忆模块**，它通过优化**记忆上下文**来克服现有模型在**长序列理解**上的局限，从而在测试时学习最优的记忆策略。实验结果表明，**ATLAS**在语言建模和长上下文理解等任务中表现优于Transformer和线性循环模型，显著**提升了性能**，论文详情请见[跳转链接](https://arxiv.org/abs/2505.23735)。
```